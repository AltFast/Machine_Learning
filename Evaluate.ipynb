{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install loralib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc713e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:15:26.760625700Z",
     "start_time": "2023-12-17T15:15:21.283950600Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_configs\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32mc:\\Users\\Ragal\\Desktop\\MDS5210-23fall-main\\latest_code\\Machine_Learning\\gpt.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gpt import GPT\n",
    "from configs import get_configs\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import tiktoken\n",
    "import json\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c1510bff1fbf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:15:41.708873900Z",
     "start_time": "2023-12-17T15:15:26.762626100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfg = get_configs(\"gpt2-medium\")\n",
    "# model = GPT.from_checkpoint(cfg, ckpt_path='sft_sft_experiment_202312141343_final.pt')\n",
    "model = GPT.from_checkpoint(cfg, ckpt_path='C:/Users/Ragal/Downloads/sft_sft_experiment_202312181738_final.pt')\n",
    "\n",
    "# model.eval()\n",
    "# model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fa1d0f46106bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:17:58.295203900Z",
     "start_time": "2023-12-17T15:17:58.279205400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_gpt2_input(prompt, device):\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "    indices = encode(prompt)\n",
    "    x = (torch.tensor(indices, dtype=torch.long, device=device)[None, ...])\n",
    "    return x, decode\n",
    "\n",
    "\n",
    "def generate_gpt2(model, prompt, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    max_new_tokens = 200\n",
    "    temperature = 0.9\n",
    "    top_k = 400\n",
    "    x, decode = prepare_gpt2_input(prompt, device)\n",
    "\n",
    "    y = model.generate(x,\n",
    "                       max_new_tokens,\n",
    "                       temperature=temperature,\n",
    "                       top_k=top_k)\n",
    "\n",
    "    res = decode(y[0].cpu().tolist())\n",
    "    end = res.find(\"<|endoftext|>\")\n",
    "    if end > 0:\n",
    "        return res[:end]\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2fcc7644835f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:26:58.112558600Z",
     "start_time": "2023-12-17T15:26:58.091560100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(\"prompts.csv\") as fp:\n",
    "#     reader = csv.DictReader(fp)\n",
    "#     prompts = [row[\"prompt\"] for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee964f7b19892a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:26:58.451259300Z",
     "start_time": "2023-12-17T15:26:58.443259400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who asked?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = 'Who asked?'\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e33107cee4247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:28:22.651588900Z",
     "start_time": "2023-12-17T15:28:00.738475Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VPN might cause network error\n",
    "print(\"Run inference\")\n",
    "if os.path.exists(\"Response.json\"):\n",
    "    with open(\"Response.json\") as fp:\n",
    "        Response = json.load(fp)\n",
    "else:\n",
    "    device = \"cuda\" # cpu very slow! use 'cuda' if you have 8g memory\n",
    "    cfg = get_configs(\"gpt2-medium\")\n",
    "    with torch.inference_mode():\n",
    "        gpt_vanilla = GPT.from_pretrained(cfg)\n",
    "        gpt_sft = model\n",
    "        Response = []\n",
    "        for prompt in tqdm(prompts):\n",
    "            Response.append({\n",
    "                \"vanilla\": generate_gpt2(gpt_vanilla, f\"Human: {prompt}\\n\\nAssistant: \", device)[\n",
    "                               len(f\"Human: {prompt}\\n\\nAssistant: \"):],\n",
    "                \"sft\": generate_gpt2(gpt_sft, f\"Human: {prompt}\\n\\nAssistant: \", device)[\n",
    "                           len(f\"Human: {prompt}\\n\\nAssistant: \"):],\n",
    "                \"prompt\": prompt\n",
    "                })\n",
    "            with open(\"Response.json\", \"w\") as fp:\n",
    "                json.dump(Response, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4272b3bf0fc93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:28:22.685586400Z",
     "start_time": "2023-12-17T15:28:22.642575200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_res = Response['vanilla']\n",
    "sft_res = Response['sft']\n",
    "question = Response['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491df574bad0e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:30:42.695297500Z",
     "start_time": "2023-12-17T15:30:42.665140Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d8ff3057b6344",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-17T15:15:45.678412400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Torch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3ed6d4f5d903a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-17T15:15:45.680412500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8bec53ea9a80e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-17T15:15:45.682413400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "reward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\n",
    "inputs1 = tokenizer(prompt, base_res, return_tensors='pt')\n",
    "score1 = rank_model(**inputs1).logits[0].cpu().detach()\n",
    "inputs2 = tokenizer(prompt, sft_res, return_tensors='pt')\n",
    "score2 = rank_model(**inputs2).logits[0].cpu().detach()\n",
    "print('Score of the basic model is {}, while the score of the sft model is {}'.format(score1, score2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
